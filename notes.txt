Things we'd need to add:

1. Support for writing XML from a Dataset. Figure this should go into the connector where WriteBatcher is used.
2. Document each MLCP feature for importing data.
- XML files
- Line-delimited JSON files
- SPARQL stuff


OOTB Spark sources:
- csv
- parquet
- jdbc
- json
- xml (in Spark 4.0; works when Spark XML connector is on classpath)
- orc = seems less important, it's a Hadoop format; user can achieve it with CustomExportCommand.
- text
- save = looks like csv/orc/text/parquet might be shortcuts for this
- saveAsTable
- insertInto

saveAsTable/insertInto don't seem quite as useful.

jdbc is clearly different from csv/parquet/orc/text/json/xml.

Per https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#saving-to-persistent-tables ,
the data is saved to a Hive metastore that is tossed into ./spark-warehouse by default.

Generic write options:
- mode
- bucketBy
- partitionBy
- sortBy


MLCP Import types:
- delimited_text
- delimited_json
- regular json/xml/text/binary files
- compressed json/xml/text/binary files
- mlcp archive of json/xml/text/binary files
- sequencefile (??? Is this a Hadoop thing we can ignore?)
- xml aggregates file
- rdf = "serialized RDF triples in one of several formats"
- forest files

Aggregate XML support
- aggregate_record_element
- aggregate_record_namespace

Delimited file support
- Custom delimiter
-
