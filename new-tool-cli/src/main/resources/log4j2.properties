# The root logger with appender name
rootLogger = INFO, STDOUT

# Assign STDOUT a valid appender & define its layout
appender.console.name = STDOUT
appender.console.type = Console
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c: %m%n

logger.marklogicclient.name=com.marklogic.client
logger.marklogicclient.level=WARN
logger.marklogicspark.name=com.marklogic.spark
logger.marklogicspark.level=INFO

# Temporary name until we decide on a real name.
logger.newtool.name=com.marklogic.newtool
logger.newtool.level=INFO

# This logs write failures at the "warn" level in a verbose way.
logger.writeBatcherImpl.name=com.marklogic.client.datamovement.impl.WriteBatcherImpl
logger.writeBatcherImpl.level=ERROR

# The following both log errors at the ERROR level, but that is usually too verbose. The user can instead include
# "--stacktrace" to see a single stacktrace when the tool catches an Exception.
logger.sparkutils.name=org.apache.spark.util.Utils
logger.sparkutils.level=FATAL
logger.sparkexecutor.name=org.apache.spark.executor.Executor
logger.sparkexecutor.level=FATAL
logger.sparktasksetmanager.name=org.apache.spark.scheduler.TaskSetManager
logger.sparktasksetmanager.level=FATAL

logger.sparksql.name=org.apache.spark.sql
logger.sparksql.level=FATAL

logger.spark.name=org.apache.spark
logger.spark.level=ERROR
logger.sparkproject.name=org.sparkproject
logger.sparkproject.level=WARN
logger.hadoop.name=org.apache.hadoop
logger.hadoop.level=ERROR
